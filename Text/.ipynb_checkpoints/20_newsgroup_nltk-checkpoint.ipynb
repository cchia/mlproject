{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '20_newsgroup/20news-bydate/'\n",
    "trainPath = path+'20news-bydate-train'\n",
    "testPath = path+'20news-bydate-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFolders = glob.glob(trainPath+'/*')\n",
    "testFolders = glob.glob(testPath+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textClass = []\n",
    "trainClassArticles = {}\n",
    "testClassArticles = {}\n",
    "for p in trainFolders:\n",
    "    textClass.append(p.split('/')[-1])\n",
    "    trainClassArticles[p.split('/')[-1]] = glob.glob(p+'/*')\n",
    "\n",
    "for p in testFolders:\n",
    "#    textClass.append(p.split('/')[-1])\n",
    "    testClassArticles[p.split('/')[-1]] = glob.glob(p+'/*')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '20_newsgroup/20news-bydate/20news-bydate-train/talk.politics.mideast/76248'\n",
    "# fin = open(path, encoding=\"latin1\")\n",
    "# text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuations = (string.punctuation).replace(\"'\", \"\") \n",
    "# text2 = text.replace('\\n', '').replace('\\t', '').lower()\n",
    "# text2 = re.sub(r' \\d+ ', 'NUM', text2)\n",
    "# text2 = text2.translate(str.maketrans(\"\", \"\", punctuations)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_word = word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_word = [word for word in tokenized_word if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_word = [stemmer.stem(word) for word in tokenized_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData, testData = [], []\n",
    "# trainLabel, testLabel = [], []\n",
    "trainData, testData = {}, {}\n",
    "trainLabel, testLabel = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = (string.punctuation).replace(\"'\", \"\") \n",
    "stop_words = stopwords.words('english')\n",
    "stemmer= PorterStemmer()\n",
    "def wordPreprocess(articles, data, label):\n",
    "    \n",
    "    data['tokenized'] = []\n",
    "    data['stop_word'] = []\n",
    "    data['stemmed'] = []\n",
    "    \n",
    "    for k, v in articles.items():\n",
    "        for path in v:\n",
    "\n",
    "            doc = []\n",
    "            with open(path, encoding=\"latin1\") as fin:\n",
    "                text = fin.read()\n",
    "                ## change to lowercase\n",
    "                lowered = text.replace('\\n', ' ').replace('\\t', ' ').lower()\n",
    "                ## replace any numerical token with NUM\n",
    "                numsub = re.sub(r' \\d+ ', 'NUM', lowered)\n",
    "                ## remove all punctuation except \"'\" because it may change the meaning\n",
    "                nopunc = numsub.translate(str.maketrans(\"\", \"\", punctuations)).strip()\n",
    "                ## tokenize words\n",
    "                tokenized_word = word_tokenize(nopunc)\n",
    "                ## collect data for tokenized words\n",
    "                data['tokenized'].append(' '.join(tokenized_word))\n",
    "                ## remove all stop words\n",
    "                stop_word = [word for word in tokenized_word if not word in stop_words]\n",
    "                ## collect tokenized words after removing stop words\n",
    "                data['stop_word'].append(' '.join(stop_word))\n",
    "                ## word stemming\n",
    "                stemmed_word = [stemmer.stem(word) for word in stop_word]\n",
    "                ## collect tokenized words after removing stop words and stemming\n",
    "                data['stemmed'].append(' '.join(stemmed_word))\n",
    "                ##data.append(' '.join(stemmed_word))\n",
    "                label.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPreprocess(trainClassArticles, trainData, trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPreprocess(testClassArticles, testData, testLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Testing Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='latin1', stop_words='english')\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = {}, {}\n",
    "y_train, y_test = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['tokenized'] = vectorizer.fit_transform(doc for doc in trainData['tokenized'])\n",
    "x_test['tokenized'] = vectorizer.transform(doc for doc in testData['tokenized'])\n",
    "\n",
    "x_train['stop_word'] = vectorizer.fit_transform(doc for doc in trainData['stop_word'])\n",
    "x_test['stop_word'] = vectorizer.transform(doc for doc in testData['stop_word'])\n",
    "\n",
    "x_train['stemmed'] = vectorizer.fit_transform(doc for doc in trainData['stemmed'])\n",
    "x_test['stemmed'] = vectorizer.transform(doc for doc in testData['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['tokenized'] = le.fit_transform(trainLabel)\n",
    "y_test['tokenized'] = le.transform(testLabel)\n",
    "y_train['stop_word'] = le.fit_transform(trainLabel)\n",
    "y_test['stop_word'] = le.transform(testLabel)\n",
    "y_train['stemmed'] = le.fit_transform(trainLabel)\n",
    "y_test['stemmed'] = le.transform(testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = vectorizer.fit_transform(doc for doc in trainData)\n",
    "# x_test = vectorizer.transform(doc for doc in testData)\n",
    "# y_train = le.fit_transform(trainLabel)\n",
    "# y_test = le.transform(testLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train with tokenized words (without removing stop words and stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleTrainData(x_train, y_train):\n",
    "    shuffleArr = list(zip(x_train.toarray(), y_train))\n",
    "    shuffle(shuffleArr)\n",
    "    return zip(*shuffleArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['tokenized'], y_train['tokenized'] = shuffleTrainData(x_train['tokenized'], y_train['tokenized'])\n",
    "x_train['stop_word'], y_train['stop_word'] = shuffleTrainData(x_train['stop_word'], y_train['stop_word'])\n",
    "x_train['stemmed'], y_train['stemmed'] = shuffleTrainData(x_train['stemmed'], y_train['stemmed'])\n",
    "#x_train, y_train = shuffleTrainData(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking with Ridge Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare modeling performance on different training sets -- tokenized, removing stop-words, and after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = RidgeClassifier()\n",
    "rc2 = RidgeClassifier()\n",
    "rc3 = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.fit(x_train['tokenized'], y_train['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974368039596959"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.score(x_train['tokenized'], y_train['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505045140732873"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.score(x_test['tokenized'], y_test['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc2.fit(x_train['stop_word'], y_train['stop_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974368039596959"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc2.score(x_train['stop_word'], y_train['stop_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503717472118959"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc2.score(x_test['stop_word'], y_test['stop_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc3.fit(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973484178893406"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc3.score(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8522304832713755"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc3.score(x_test['stemmed'], y_test['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference by just removing stop-words, however, prediction on test set is slightly better after applying stemming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try different model on 'stemmed' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.fit(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993812975075128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.score(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198353690918747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.score(x_test['stemmed'], y_test['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505921866713806"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8131970260223048"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(x_test['stemmed'], y_test['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976135761004066"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6107275624004248"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(x_test['stemmed'], y_test['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992045253668022"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_train['stemmed'], y_train['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506372809346787"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_test['stemmed'], y_test['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
